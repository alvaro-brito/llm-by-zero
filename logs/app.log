2025-05-17 22:01:50,423 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-17 22:01:50,433 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-17 22:02:07,222 - app - INFO - [llm_controller.py:73] - Model 52 created and training started
2025-05-17 22:02:07,241 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-17 22:02:07,652 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-17 22:02:07,669 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-17 22:02:07,680 - app - INFO - [model_trainer.py:91] - Train data size: 23973, Validation data size: 2664
2025-05-17 22:05:44,337 - app - ERROR - [model_trainer.py:172] - Error training model: Object of type Tensor is not JSON serializable
2025-05-17 22:05:44,361 - app - ERROR - [llm_controller.py:37] - Training failed for model 52: Object of type Tensor is not JSON serializable
2025-05-17 22:06:16,656 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-17 22:06:16,666 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-17 22:06:17,691 - app - INFO - [llm_controller.py:73] - Model 53 created and training started
2025-05-17 22:06:17,710 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-17 22:06:18,082 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-17 22:06:18,097 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-17 22:06:18,112 - app - INFO - [model_trainer.py:91] - Train data size: 23973, Validation data size: 2664
2025-05-17 22:16:18,758 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-17 22:16:18,770 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-17 22:16:24,709 - app - INFO - [llm_controller.py:73] - Model 54 created and training started
2025-05-17 22:16:24,724 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-17 22:16:25,058 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-17 22:16:25,076 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-17 22:16:25,093 - app - INFO - [model_trainer.py:91] - Train data size: 23973, Validation data size: 2664
2025-05-17 22:19:20,185 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-17 22:19:20,201 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-17 22:19:31,506 - app - INFO - [llm_controller.py:73] - Model 55 created and training started
2025-05-17 22:19:31,522 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-17 22:19:31,857 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-17 22:19:31,873 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-17 22:19:31,884 - app - INFO - [model_trainer.py:91] - Train data size: 23973, Validation data size: 2664
2025-05-17 22:47:42,004 - app - INFO - [model_trainer.py:151] - Early stopping triggered
2025-05-17 22:47:42,504 - app - ERROR - [llm_controller.py:37] - Training failed for model 55: Object of type Tensor is not JSON serializable
2025-05-17 23:54:18,680 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-17 23:54:18,689 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-17 23:54:23,794 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-17 23:54:23,804 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-17 23:54:24,463 - app - INFO - [llm_controller.py:73] - Model 56 created and training started
2025-05-17 23:54:24,477 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-17 23:54:24,848 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-17 23:54:24,862 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-17 23:54:24,876 - app - INFO - [model_trainer.py:91] - Train data size: 23973, Validation data size: 2664
2025-05-17 23:56:43,515 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-17 23:56:43,527 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-17 23:56:51,551 - app - INFO - [llm_controller.py:73] - Model 57 created and training started
2025-05-17 23:56:51,572 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-17 23:56:51,944 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-17 23:56:51,961 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-17 23:56:51,980 - app - INFO - [model_trainer.py:91] - Train data size: 23973, Validation data size: 2664
2025-05-18 00:22:08,903 - app - INFO - [model_trainer.py:151] - Early stopping triggered
2025-05-18 00:22:19,226 - app - INFO - [inference_service.py:20] - Initializing InferenceService
2025-05-18 00:22:19,227 - app - INFO - [inference_service.py:23] - Using device: cpu
2025-05-18 00:22:19,236 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:22:20,014 - app - INFO - [inference_service.py:50] - Tokenizer initialized
2025-05-18 00:22:20,015 - app - INFO - [inference_service.py:55] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:22:20,431 - app - INFO - [inference_service.py:64] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmpy1xotxip.pt
2025-05-18 00:22:20,451 - app - INFO - [inference_service.py:68] - Model checkpoint loaded
2025-05-18 00:22:20,451 - app - INFO - [inference_service.py:72] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:22:20,451 - app - INFO - [inference_service.py:76] - Model vocabulary size: 605
2025-05-18 00:22:20,558 - app - INFO - [inference_service.py:88] - Model initialized
2025-05-18 00:22:20,567 - app - INFO - [inference_service.py:93] - Model weights loaded
2025-05-18 00:22:20,569 - app - INFO - [inference_service.py:105] - Temporary file cleaned up
2025-05-18 00:22:20,575 - app - INFO - [inference_service.py:128] - Generating response for model 57
2025-05-18 00:22:20,576 - app - INFO - [inference_service.py:129] - Prompt: Machine learning is
2025-05-18 00:22:20,576 - app - INFO - [inference_service.py:130] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:22:20,576 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:22:20,576 - app - INFO - [inference_service.py:35] - Model already loaded
2025-05-18 00:22:20,577 - app - INFO - [inference_service.py:144] - Encoding input
2025-05-18 00:22:20,579 - app - INFO - [inference_service.py:147] - Input shape: torch.Size([1, 3])
2025-05-18 00:22:20,580 - app - INFO - [inference_service.py:154] - Generating response
2025-05-18 00:22:20,632 - app - ERROR - [inference_service.py:191] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 163, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:22:20,638 - app - INFO - [inference_service.py:128] - Generating response for model 57
2025-05-18 00:22:20,639 - app - INFO - [inference_service.py:129] - Prompt: Neural networks can
2025-05-18 00:22:20,639 - app - INFO - [inference_service.py:130] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:22:20,639 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:22:20,639 - app - INFO - [inference_service.py:35] - Model already loaded
2025-05-18 00:22:20,639 - app - INFO - [inference_service.py:144] - Encoding input
2025-05-18 00:22:20,640 - app - INFO - [inference_service.py:147] - Input shape: torch.Size([1, 4])
2025-05-18 00:22:20,640 - app - INFO - [inference_service.py:154] - Generating response
2025-05-18 00:22:20,677 - app - ERROR - [inference_service.py:191] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 163, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:22:20,678 - app - INFO - [inference_service.py:128] - Generating response for model 57
2025-05-18 00:22:20,679 - app - INFO - [inference_service.py:129] - Prompt: Deep learning enables
2025-05-18 00:22:20,679 - app - INFO - [inference_service.py:130] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:22:20,680 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:22:20,680 - app - INFO - [inference_service.py:35] - Model already loaded
2025-05-18 00:22:20,680 - app - INFO - [inference_service.py:144] - Encoding input
2025-05-18 00:22:20,681 - app - INFO - [inference_service.py:147] - Input shape: torch.Size([1, 3])
2025-05-18 00:22:20,682 - app - INFO - [inference_service.py:154] - Generating response
2025-05-18 00:22:20,724 - app - ERROR - [inference_service.py:191] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 163, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:22:20,725 - app - INFO - [inference_service.py:128] - Generating response for model 57
2025-05-18 00:22:20,725 - app - INFO - [inference_service.py:129] - Prompt: Reinforcement learning allows
2025-05-18 00:22:20,725 - app - INFO - [inference_service.py:130] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:22:20,726 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:22:20,726 - app - INFO - [inference_service.py:35] - Model already loaded
2025-05-18 00:22:20,726 - app - INFO - [inference_service.py:144] - Encoding input
2025-05-18 00:22:20,727 - app - INFO - [inference_service.py:147] - Input shape: torch.Size([1, 5])
2025-05-18 00:22:20,727 - app - INFO - [inference_service.py:154] - Generating response
2025-05-18 00:22:20,778 - app - ERROR - [inference_service.py:191] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 163, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:22:20,779 - app - INFO - [inference_service.py:128] - Generating response for model 57
2025-05-18 00:22:20,779 - app - INFO - [inference_service.py:129] - Prompt: Natural language processing helps
2025-05-18 00:22:20,780 - app - INFO - [inference_service.py:130] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:22:20,780 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:22:20,780 - app - INFO - [inference_service.py:35] - Model already loaded
2025-05-18 00:22:20,781 - app - INFO - [inference_service.py:144] - Encoding input
2025-05-18 00:22:20,782 - app - INFO - [inference_service.py:147] - Input shape: torch.Size([1, 4])
2025-05-18 00:22:20,782 - app - INFO - [inference_service.py:154] - Generating response
2025-05-18 00:22:20,809 - app - ERROR - [inference_service.py:191] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 163, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:22:20,877 - app - INFO - [inference_service.py:20] - Initializing InferenceService
2025-05-18 00:22:20,878 - app - INFO - [inference_service.py:23] - Using device: cpu
2025-05-18 00:22:20,882 - app - INFO - [llm_controller.py:111] - Received inference request for model 57
2025-05-18 00:22:20,883 - app - INFO - [inference_service.py:128] - Generating response for model 57
2025-05-18 00:22:20,883 - app - INFO - [inference_service.py:129] - Prompt: Machine learning is a field of study that
2025-05-18 00:22:20,884 - app - INFO - [inference_service.py:130] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:22:20,884 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:22:21,344 - app - INFO - [inference_service.py:50] - Tokenizer initialized
2025-05-18 00:22:21,345 - app - INFO - [inference_service.py:55] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:22:21,770 - app - INFO - [inference_service.py:64] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmpt66bdzu4.pt
2025-05-18 00:22:21,791 - app - INFO - [inference_service.py:68] - Model checkpoint loaded
2025-05-18 00:22:21,792 - app - INFO - [inference_service.py:72] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:22:21,792 - app - INFO - [inference_service.py:76] - Model vocabulary size: 605
2025-05-18 00:22:21,898 - app - INFO - [inference_service.py:88] - Model initialized
2025-05-18 00:22:21,906 - app - INFO - [inference_service.py:93] - Model weights loaded
2025-05-18 00:22:21,908 - app - INFO - [inference_service.py:105] - Temporary file cleaned up
2025-05-18 00:22:21,912 - app - INFO - [inference_service.py:144] - Encoding input
2025-05-18 00:22:21,912 - app - INFO - [inference_service.py:147] - Input shape: torch.Size([1, 8])
2025-05-18 00:22:21,912 - app - INFO - [inference_service.py:154] - Generating response
2025-05-18 00:22:21,956 - app - ERROR - [inference_service.py:191] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 163, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:22:21,957 - app - ERROR - [llm_controller.py:122] - Inference error: Error generating response: index out of range in self
2025-05-18 00:22:21,957 - app - ERROR - [llm_controller.py:133] - Error during inference: 500: Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/controller/llm_controller.py", line 123, in generate_inference
    raise HTTPException(status_code=500, detail=response["error"])
fastapi.exceptions.HTTPException: 500: Error generating response: index out of range in self
2025-05-18 00:34:16,533 - app - INFO - [inference_service.py:20] - Initializing InferenceService
2025-05-18 00:34:16,534 - app - INFO - [inference_service.py:23] - Using device: cpu
2025-05-18 00:34:16,535 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:34:17,259 - app - INFO - [inference_service.py:50] - Tokenizer initialized
2025-05-18 00:34:17,259 - app - INFO - [inference_service.py:55] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:34:17,655 - app - INFO - [inference_service.py:64] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmp_9ljq8ya.pt
2025-05-18 00:34:17,674 - app - INFO - [inference_service.py:68] - Model checkpoint loaded
2025-05-18 00:34:17,675 - app - INFO - [inference_service.py:72] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:34:17,675 - app - INFO - [inference_service.py:76] - Model vocabulary size: 605
2025-05-18 00:34:17,751 - app - INFO - [inference_service.py:88] - Model initialized
2025-05-18 00:34:17,759 - app - INFO - [inference_service.py:93] - Model weights loaded
2025-05-18 00:34:17,761 - app - INFO - [inference_service.py:105] - Temporary file cleaned up
2025-05-18 00:34:17,763 - app - INFO - [inference_service.py:128] - Generating response for model 57
2025-05-18 00:34:17,764 - app - INFO - [inference_service.py:129] - Prompt: Machine learning is
2025-05-18 00:34:17,764 - app - INFO - [inference_service.py:130] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:34:17,764 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:34:17,764 - app - INFO - [inference_service.py:35] - Model already loaded
2025-05-18 00:34:17,764 - app - INFO - [inference_service.py:144] - Encoding input
2025-05-18 00:34:17,765 - app - INFO - [inference_service.py:147] - Input shape: torch.Size([1, 3])
2025-05-18 00:34:17,765 - app - INFO - [inference_service.py:154] - Generating response
2025-05-18 00:34:17,794 - app - ERROR - [inference_service.py:191] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 163, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:34:17,795 - app - INFO - [inference_service.py:128] - Generating response for model 57
2025-05-18 00:34:17,795 - app - INFO - [inference_service.py:129] - Prompt: Neural networks can
2025-05-18 00:34:17,795 - app - INFO - [inference_service.py:130] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:34:17,795 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:34:17,796 - app - INFO - [inference_service.py:35] - Model already loaded
2025-05-18 00:34:17,796 - app - INFO - [inference_service.py:144] - Encoding input
2025-05-18 00:34:17,796 - app - INFO - [inference_service.py:147] - Input shape: torch.Size([1, 4])
2025-05-18 00:34:17,796 - app - INFO - [inference_service.py:154] - Generating response
2025-05-18 00:34:17,829 - app - ERROR - [inference_service.py:191] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 163, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:34:17,829 - app - INFO - [inference_service.py:128] - Generating response for model 57
2025-05-18 00:34:17,830 - app - INFO - [inference_service.py:129] - Prompt: Deep learning enables
2025-05-18 00:34:17,830 - app - INFO - [inference_service.py:130] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:34:17,830 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:34:17,830 - app - INFO - [inference_service.py:35] - Model already loaded
2025-05-18 00:34:17,830 - app - INFO - [inference_service.py:144] - Encoding input
2025-05-18 00:34:17,831 - app - INFO - [inference_service.py:147] - Input shape: torch.Size([1, 3])
2025-05-18 00:34:17,831 - app - INFO - [inference_service.py:154] - Generating response
2025-05-18 00:34:17,864 - app - ERROR - [inference_service.py:191] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 163, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:34:17,865 - app - INFO - [inference_service.py:128] - Generating response for model 57
2025-05-18 00:34:17,865 - app - INFO - [inference_service.py:129] - Prompt: Reinforcement learning allows
2025-05-18 00:34:17,866 - app - INFO - [inference_service.py:130] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:34:17,866 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:34:17,866 - app - INFO - [inference_service.py:35] - Model already loaded
2025-05-18 00:34:17,866 - app - INFO - [inference_service.py:144] - Encoding input
2025-05-18 00:34:17,867 - app - INFO - [inference_service.py:147] - Input shape: torch.Size([1, 5])
2025-05-18 00:34:17,867 - app - INFO - [inference_service.py:154] - Generating response
2025-05-18 00:34:17,898 - app - ERROR - [inference_service.py:191] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 163, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:34:17,899 - app - INFO - [inference_service.py:128] - Generating response for model 57
2025-05-18 00:34:17,899 - app - INFO - [inference_service.py:129] - Prompt: Natural language processing helps
2025-05-18 00:34:17,899 - app - INFO - [inference_service.py:130] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:34:17,899 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:34:17,899 - app - INFO - [inference_service.py:35] - Model already loaded
2025-05-18 00:34:17,900 - app - INFO - [inference_service.py:144] - Encoding input
2025-05-18 00:34:17,900 - app - INFO - [inference_service.py:147] - Input shape: torch.Size([1, 4])
2025-05-18 00:34:17,900 - app - INFO - [inference_service.py:154] - Generating response
2025-05-18 00:34:17,930 - app - ERROR - [inference_service.py:191] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 163, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:34:17,996 - app - INFO - [inference_service.py:20] - Initializing InferenceService
2025-05-18 00:34:17,996 - app - INFO - [inference_service.py:23] - Using device: cpu
2025-05-18 00:34:17,996 - app - INFO - [llm_controller.py:111] - Received inference request for model 57
2025-05-18 00:34:17,996 - app - INFO - [inference_service.py:128] - Generating response for model 57
2025-05-18 00:34:17,997 - app - INFO - [inference_service.py:129] - Prompt: Machine learning is a field of study that
2025-05-18 00:34:17,997 - app - INFO - [inference_service.py:130] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:34:17,997 - app - INFO - [inference_service.py:31] - Loading model 57
2025-05-18 00:34:18,426 - app - INFO - [inference_service.py:50] - Tokenizer initialized
2025-05-18 00:34:18,426 - app - INFO - [inference_service.py:55] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:34:18,757 - app - INFO - [inference_service.py:64] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmp9uzaem1p.pt
2025-05-18 00:34:18,773 - app - INFO - [inference_service.py:68] - Model checkpoint loaded
2025-05-18 00:34:18,774 - app - INFO - [inference_service.py:72] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:34:18,774 - app - INFO - [inference_service.py:76] - Model vocabulary size: 605
2025-05-18 00:34:18,855 - app - INFO - [inference_service.py:88] - Model initialized
2025-05-18 00:34:18,864 - app - INFO - [inference_service.py:93] - Model weights loaded
2025-05-18 00:34:18,865 - app - INFO - [inference_service.py:105] - Temporary file cleaned up
2025-05-18 00:34:18,869 - app - INFO - [inference_service.py:144] - Encoding input
2025-05-18 00:34:18,870 - app - INFO - [inference_service.py:147] - Input shape: torch.Size([1, 8])
2025-05-18 00:34:18,870 - app - INFO - [inference_service.py:154] - Generating response
2025-05-18 00:34:18,903 - app - ERROR - [inference_service.py:191] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 163, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:34:18,904 - app - ERROR - [llm_controller.py:122] - Inference error: Error generating response: index out of range in self
2025-05-18 00:34:18,904 - app - ERROR - [llm_controller.py:133] - Error during inference: 500: Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/controller/llm_controller.py", line 123, in generate_inference
    raise HTTPException(status_code=500, detail=response["error"])
fastapi.exceptions.HTTPException: 500: Error generating response: index out of range in self
2025-05-18 00:35:44,699 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:35:44,710 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:36:33,267 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:36:33,279 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:36:39,567 - app - INFO - [inference_service.py:21] - Initializing InferenceService
2025-05-18 00:36:39,568 - app - INFO - [inference_service.py:24] - Using device: cpu
2025-05-18 00:36:39,568 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 00:36:39,888 - app - INFO - [inference_service.py:33] - Loading model 57
2025-05-18 00:36:39,895 - app - INFO - [inference_service.py:52] - Using training tokenizer
2025-05-18 00:36:39,896 - app - INFO - [inference_service.py:57] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:36:40,258 - app - INFO - [inference_service.py:66] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmpdaf3es9p.pt
2025-05-18 00:36:40,279 - app - INFO - [inference_service.py:70] - Model checkpoint loaded
2025-05-18 00:36:40,279 - app - INFO - [inference_service.py:74] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:36:41,209 - app - INFO - [inference_service.py:86] - Model initialized
2025-05-18 00:36:41,217 - app - ERROR - [inference_service.py:97] - Error loading model from file: Error(s) in loading state_dict for TransformerLanguageModel:
	size mismatch for token_embedding_table.weight: copying a param with shape torch.Size([605, 256]) from checkpoint, the shape in current model is torch.Size([100277, 256]).
	size mismatch for lm_head.weight: copying a param with shape torch.Size([605, 256]) from checkpoint, the shape in current model is torch.Size([100277, 256]).
	size mismatch for lm_head.bias: copying a param with shape torch.Size([605]) from checkpoint, the shape in current model is torch.Size([100277]).
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 89, in load_model
    self.model.load_state_dict(checkpoint['model_state_dict'])
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2153, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for TransformerLanguageModel:
	size mismatch for token_embedding_table.weight: copying a param with shape torch.Size([605, 256]) from checkpoint, the shape in current model is torch.Size([100277, 256]).
	size mismatch for lm_head.weight: copying a param with shape torch.Size([605, 256]) from checkpoint, the shape in current model is torch.Size([100277, 256]).
	size mismatch for lm_head.bias: copying a param with shape torch.Size([605]) from checkpoint, the shape in current model is torch.Size([100277]).
2025-05-18 00:36:41,223 - app - INFO - [inference_service.py:103] - Temporary file cleaned up
2025-05-18 00:36:41,225 - app - ERROR - [llm_controller.py:164] - Error during evaluation: 500: Failed to load model
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/controller/llm_controller.py", line 156, in evaluate_model
    raise HTTPException(status_code=500, detail=results["error"])
fastapi.exceptions.HTTPException: 500: Failed to load model
2025-05-18 00:36:41,289 - app - INFO - [inference_service.py:21] - Initializing InferenceService
2025-05-18 00:36:41,289 - app - INFO - [inference_service.py:24] - Using device: cpu
2025-05-18 00:36:41,289 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 00:36:41,294 - app - INFO - [llm_controller.py:111] - Received inference request for model 57
2025-05-18 00:36:41,294 - app - INFO - [inference_service.py:126] - Generating response for model 57
2025-05-18 00:36:41,294 - app - INFO - [inference_service.py:127] - Prompt: Machine learning is a field of study that
2025-05-18 00:36:41,295 - app - INFO - [inference_service.py:128] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:36:41,295 - app - INFO - [inference_service.py:33] - Loading model 57
2025-05-18 00:36:41,303 - app - INFO - [inference_service.py:52] - Using training tokenizer
2025-05-18 00:36:41,303 - app - INFO - [inference_service.py:57] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:36:41,653 - app - INFO - [inference_service.py:66] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmp4os2o01y.pt
2025-05-18 00:36:41,674 - app - INFO - [inference_service.py:70] - Model checkpoint loaded
2025-05-18 00:36:41,674 - app - INFO - [inference_service.py:74] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:36:42,529 - app - INFO - [inference_service.py:86] - Model initialized
2025-05-18 00:36:42,539 - app - ERROR - [inference_service.py:97] - Error loading model from file: Error(s) in loading state_dict for TransformerLanguageModel:
	size mismatch for token_embedding_table.weight: copying a param with shape torch.Size([605, 256]) from checkpoint, the shape in current model is torch.Size([100277, 256]).
	size mismatch for lm_head.weight: copying a param with shape torch.Size([605, 256]) from checkpoint, the shape in current model is torch.Size([100277, 256]).
	size mismatch for lm_head.bias: copying a param with shape torch.Size([605]) from checkpoint, the shape in current model is torch.Size([100277]).
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 89, in load_model
    self.model.load_state_dict(checkpoint['model_state_dict'])
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2153, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for TransformerLanguageModel:
	size mismatch for token_embedding_table.weight: copying a param with shape torch.Size([605, 256]) from checkpoint, the shape in current model is torch.Size([100277, 256]).
	size mismatch for lm_head.weight: copying a param with shape torch.Size([605, 256]) from checkpoint, the shape in current model is torch.Size([100277, 256]).
	size mismatch for lm_head.bias: copying a param with shape torch.Size([605]) from checkpoint, the shape in current model is torch.Size([100277]).
2025-05-18 00:36:42,542 - app - INFO - [inference_service.py:103] - Temporary file cleaned up
2025-05-18 00:36:42,545 - app - ERROR - [inference_service.py:135] - Failed to load model
2025-05-18 00:36:42,545 - app - ERROR - [llm_controller.py:122] - Inference error: Failed to load model
2025-05-18 00:36:42,545 - app - ERROR - [llm_controller.py:133] - Error during inference: 500: Failed to load model
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/controller/llm_controller.py", line 123, in generate_inference
    raise HTTPException(status_code=500, detail=response["error"])
fastapi.exceptions.HTTPException: 500: Failed to load model
2025-05-18 00:38:00,779 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:38:00,790 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:38:14,752 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:38:14,763 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:38:40,127 - app - INFO - [inference_service.py:21] - Initializing InferenceService
2025-05-18 00:38:40,128 - app - INFO - [inference_service.py:24] - Using device: cpu
2025-05-18 00:38:40,128 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 00:38:40,415 - app - INFO - [inference_service.py:33] - Loading model 57
2025-05-18 00:38:40,422 - app - INFO - [inference_service.py:53] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:38:40,767 - app - INFO - [inference_service.py:62] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmpoard14nr.pt
2025-05-18 00:38:40,784 - app - INFO - [inference_service.py:66] - Model checkpoint loaded
2025-05-18 00:38:40,784 - app - INFO - [inference_service.py:70] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:38:40,785 - app - INFO - [inference_service.py:74] - Model vocabulary size from checkpoint: 605
2025-05-18 00:38:40,868 - app - INFO - [inference_service.py:86] - Model initialized
2025-05-18 00:38:40,877 - app - INFO - [inference_service.py:91] - Model weights loaded
2025-05-18 00:38:40,878 - app - INFO - [inference_service.py:95] - Using training tokenizer
2025-05-18 00:38:40,879 - app - INFO - [inference_service.py:107] - Temporary file cleaned up
2025-05-18 00:38:40,881 - app - INFO - [inference_service.py:130] - Generating response for model 57
2025-05-18 00:38:40,881 - app - INFO - [inference_service.py:131] - Prompt: Machine learning is
2025-05-18 00:38:40,881 - app - INFO - [inference_service.py:132] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:38:40,882 - app - INFO - [inference_service.py:33] - Loading model 57
2025-05-18 00:38:40,882 - app - INFO - [inference_service.py:37] - Model already loaded
2025-05-18 00:38:40,882 - app - INFO - [inference_service.py:146] - Encoding input
2025-05-18 00:38:40,888 - app - INFO - [inference_service.py:149] - Input shape: torch.Size([1, 3])
2025-05-18 00:38:40,888 - app - INFO - [inference_service.py:156] - Generating response
2025-05-18 00:38:40,935 - app - ERROR - [inference_service.py:193] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 165, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:38:40,939 - app - INFO - [inference_service.py:130] - Generating response for model 57
2025-05-18 00:38:40,939 - app - INFO - [inference_service.py:131] - Prompt: Neural networks can
2025-05-18 00:38:40,939 - app - INFO - [inference_service.py:132] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:38:40,940 - app - INFO - [inference_service.py:33] - Loading model 57
2025-05-18 00:38:40,940 - app - INFO - [inference_service.py:37] - Model already loaded
2025-05-18 00:38:40,940 - app - INFO - [inference_service.py:146] - Encoding input
2025-05-18 00:38:40,941 - app - INFO - [inference_service.py:149] - Input shape: torch.Size([1, 4])
2025-05-18 00:38:40,941 - app - INFO - [inference_service.py:156] - Generating response
2025-05-18 00:38:40,976 - app - ERROR - [inference_service.py:193] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 165, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:38:40,977 - app - INFO - [inference_service.py:130] - Generating response for model 57
2025-05-18 00:38:40,977 - app - INFO - [inference_service.py:131] - Prompt: Deep learning enables
2025-05-18 00:38:40,977 - app - INFO - [inference_service.py:132] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:38:40,977 - app - INFO - [inference_service.py:33] - Loading model 57
2025-05-18 00:38:40,978 - app - INFO - [inference_service.py:37] - Model already loaded
2025-05-18 00:38:40,978 - app - INFO - [inference_service.py:146] - Encoding input
2025-05-18 00:38:40,978 - app - INFO - [inference_service.py:149] - Input shape: torch.Size([1, 3])
2025-05-18 00:38:40,978 - app - INFO - [inference_service.py:156] - Generating response
2025-05-18 00:38:41,010 - app - ERROR - [inference_service.py:193] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 165, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:38:41,011 - app - INFO - [inference_service.py:130] - Generating response for model 57
2025-05-18 00:38:41,012 - app - INFO - [inference_service.py:131] - Prompt: Reinforcement learning allows
2025-05-18 00:38:41,012 - app - INFO - [inference_service.py:132] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:38:41,012 - app - INFO - [inference_service.py:33] - Loading model 57
2025-05-18 00:38:41,013 - app - INFO - [inference_service.py:37] - Model already loaded
2025-05-18 00:38:41,013 - app - INFO - [inference_service.py:146] - Encoding input
2025-05-18 00:38:41,013 - app - INFO - [inference_service.py:149] - Input shape: torch.Size([1, 5])
2025-05-18 00:38:41,014 - app - INFO - [inference_service.py:156] - Generating response
2025-05-18 00:38:41,049 - app - ERROR - [inference_service.py:193] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 165, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:38:41,051 - app - INFO - [inference_service.py:130] - Generating response for model 57
2025-05-18 00:38:41,051 - app - INFO - [inference_service.py:131] - Prompt: Natural language processing helps
2025-05-18 00:38:41,051 - app - INFO - [inference_service.py:132] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:38:41,051 - app - INFO - [inference_service.py:33] - Loading model 57
2025-05-18 00:38:41,051 - app - INFO - [inference_service.py:37] - Model already loaded
2025-05-18 00:38:41,052 - app - INFO - [inference_service.py:146] - Encoding input
2025-05-18 00:38:41,052 - app - INFO - [inference_service.py:149] - Input shape: torch.Size([1, 4])
2025-05-18 00:38:41,052 - app - INFO - [inference_service.py:156] - Generating response
2025-05-18 00:38:41,089 - app - ERROR - [inference_service.py:193] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 165, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:38:41,238 - app - INFO - [inference_service.py:21] - Initializing InferenceService
2025-05-18 00:38:41,239 - app - INFO - [inference_service.py:24] - Using device: cpu
2025-05-18 00:38:41,239 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 00:38:41,245 - app - INFO - [llm_controller.py:111] - Received inference request for model 57
2025-05-18 00:38:41,245 - app - INFO - [inference_service.py:130] - Generating response for model 57
2025-05-18 00:38:41,245 - app - INFO - [inference_service.py:131] - Prompt: Machine learning is a field of study that
2025-05-18 00:38:41,246 - app - INFO - [inference_service.py:132] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:38:41,246 - app - INFO - [inference_service.py:33] - Loading model 57
2025-05-18 00:38:41,258 - app - INFO - [inference_service.py:53] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:38:41,708 - app - INFO - [inference_service.py:62] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmpi1477kmo.pt
2025-05-18 00:38:41,729 - app - INFO - [inference_service.py:66] - Model checkpoint loaded
2025-05-18 00:38:41,729 - app - INFO - [inference_service.py:70] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:38:41,729 - app - INFO - [inference_service.py:74] - Model vocabulary size from checkpoint: 605
2025-05-18 00:38:41,812 - app - INFO - [inference_service.py:86] - Model initialized
2025-05-18 00:38:41,822 - app - INFO - [inference_service.py:91] - Model weights loaded
2025-05-18 00:38:41,822 - app - INFO - [inference_service.py:95] - Using training tokenizer
2025-05-18 00:38:41,824 - app - INFO - [inference_service.py:107] - Temporary file cleaned up
2025-05-18 00:38:41,827 - app - INFO - [inference_service.py:146] - Encoding input
2025-05-18 00:38:41,828 - app - INFO - [inference_service.py:149] - Input shape: torch.Size([1, 8])
2025-05-18 00:38:41,828 - app - INFO - [inference_service.py:156] - Generating response
2025-05-18 00:38:41,864 - app - ERROR - [inference_service.py:193] - Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 165, in generate_response
    logits = self.model(outputs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/helpers/transformer_model.py", line 104, in forward
    tok_emb = self.token_embedding_table(idx)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-05-18 00:38:41,865 - app - ERROR - [llm_controller.py:122] - Inference error: Error generating response: index out of range in self
2025-05-18 00:38:41,866 - app - ERROR - [llm_controller.py:133] - Error during inference: 500: Error generating response: index out of range in self
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/controller/llm_controller.py", line 123, in generate_inference
    raise HTTPException(status_code=500, detail=response["error"])
fastapi.exceptions.HTTPException: 500: Error generating response: index out of range in self
2025-05-18 00:41:17,197 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:41:17,207 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:41:31,567 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:41:31,579 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:41:57,748 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:41:57,759 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:41:58,676 - app - INFO - [inference_service.py:22] - Initializing InferenceService
2025-05-18 00:41:58,676 - app - INFO - [inference_service.py:25] - Using device: cpu
2025-05-18 00:41:58,680 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:41:58,690 - app - INFO - [inference_service.py:54] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:41:59,063 - app - INFO - [inference_service.py:63] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmp7ejo8237.pt
2025-05-18 00:41:59,082 - app - INFO - [inference_service.py:67] - Model checkpoint loaded
2025-05-18 00:41:59,083 - app - INFO - [inference_service.py:71] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:41:59,083 - app - INFO - [inference_service.py:75] - Model vocabulary size from checkpoint: 605
2025-05-18 00:41:59,171 - app - INFO - [inference_service.py:87] - Model initialized
2025-05-18 00:41:59,183 - app - INFO - [inference_service.py:92] - Model weights loaded
2025-05-18 00:41:59,184 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 00:41:59,492 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-18 00:41:59,505 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-18 00:41:59,517 - app - INFO - [model_trainer.py:103] - Train data size: 23973, Validation data size: 2664
2025-05-18 00:41:59,518 - app - INFO - [inference_service.py:104] - Using custom tokenizer with vocab size: 605
2025-05-18 00:41:59,519 - app - INFO - [inference_service.py:116] - Temporary file cleaned up
2025-05-18 00:41:59,522 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:41:59,522 - app - INFO - [inference_service.py:166] - Prompt: Machine learning is
2025-05-18 00:41:59,522 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:41:59,522 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:41:59,523 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:41:59,523 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:41:59,523 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 3])
2025-05-18 00:41:59,524 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:42:00,730 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:42:00,730 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:42:00,731 - app - INFO - [inference_service.py:166] - Prompt: Neural networks can
2025-05-18 00:42:00,731 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:42:00,731 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:42:00,731 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:42:00,732 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:42:00,732 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 3])
2025-05-18 00:42:00,732 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:42:01,959 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:42:01,960 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:42:01,960 - app - INFO - [inference_service.py:166] - Prompt: Deep learning enables
2025-05-18 00:42:01,960 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:42:01,960 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:42:01,960 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:42:01,960 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:42:01,961 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 3])
2025-05-18 00:42:01,961 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:42:02,936 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:42:02,936 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:42:02,936 - app - INFO - [inference_service.py:166] - Prompt: Reinforcement learning allows
2025-05-18 00:42:02,937 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:42:02,937 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:42:02,937 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:42:02,937 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:42:02,937 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 8])
2025-05-18 00:42:02,937 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:42:03,846 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:42:03,846 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:42:03,846 - app - INFO - [inference_service.py:166] - Prompt: Natural language processing helps
2025-05-18 00:42:03,847 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:42:03,847 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:42:03,847 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:42:03,847 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:42:03,848 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 14])
2025-05-18 00:42:03,848 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:42:04,707 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:42:04,707 - app - ERROR - [inference_service.py:336] - Error evaluating model: 'CustomTokenizer' object is not callable
2025-05-18 00:42:04,708 - app - ERROR - [llm_controller.py:164] - Error during evaluation: 500: 'CustomTokenizer' object is not callable
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/controller/llm_controller.py", line 156, in evaluate_model
    raise HTTPException(status_code=500, detail=results["error"])
fastapi.exceptions.HTTPException: 500: 'CustomTokenizer' object is not callable
2025-05-18 00:42:09,286 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:42:09,296 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:42:09,312 - app - INFO - [inference_service.py:22] - Initializing InferenceService
2025-05-18 00:42:09,312 - app - INFO - [inference_service.py:25] - Using device: cpu
2025-05-18 00:42:09,315 - app - INFO - [llm_controller.py:111] - Received inference request for model 57
2025-05-18 00:42:09,316 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:42:09,316 - app - INFO - [inference_service.py:166] - Prompt: Machine learning is a field of study that
2025-05-18 00:42:09,316 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:42:09,316 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:42:09,332 - app - INFO - [inference_service.py:54] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:42:09,697 - app - INFO - [inference_service.py:63] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmpdox_vjdq.pt
2025-05-18 00:42:09,717 - app - INFO - [inference_service.py:67] - Model checkpoint loaded
2025-05-18 00:42:09,717 - app - INFO - [inference_service.py:71] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:42:09,717 - app - INFO - [inference_service.py:75] - Model vocabulary size from checkpoint: 605
2025-05-18 00:42:09,809 - app - INFO - [inference_service.py:87] - Model initialized
2025-05-18 00:42:09,817 - app - INFO - [inference_service.py:92] - Model weights loaded
2025-05-18 00:42:09,818 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 00:42:10,119 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-18 00:42:10,131 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-18 00:42:10,144 - app - INFO - [model_trainer.py:103] - Train data size: 23973, Validation data size: 2664
2025-05-18 00:42:10,144 - app - INFO - [inference_service.py:104] - Using custom tokenizer with vocab size: 605
2025-05-18 00:42:10,146 - app - INFO - [inference_service.py:116] - Temporary file cleaned up
2025-05-18 00:42:10,148 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:42:10,148 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 12])
2025-05-18 00:42:10,149 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:42:11,107 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:42:11,107 - app - INFO - [llm_controller.py:125] - Inference completed successfully
2025-05-18 00:42:11,108 - app - ERROR - [llm_controller.py:133] - Error during inference: 'metadata'
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/controller/llm_controller.py", line 129, in generate_inference
    metadata=response["metadata"]
KeyError: 'metadata'
2025-05-18 00:42:35,109 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:42:35,119 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:42:55,263 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:42:55,278 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:42:55,363 - app - INFO - [inference_service.py:22] - Initializing InferenceService
2025-05-18 00:42:55,364 - app - INFO - [inference_service.py:25] - Using device: cpu
2025-05-18 00:42:55,368 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:42:55,377 - app - INFO - [inference_service.py:54] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:42:55,755 - app - INFO - [inference_service.py:63] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmpukbpswq5.pt
2025-05-18 00:42:55,775 - app - INFO - [inference_service.py:67] - Model checkpoint loaded
2025-05-18 00:42:55,776 - app - INFO - [inference_service.py:71] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:42:55,776 - app - INFO - [inference_service.py:75] - Model vocabulary size from checkpoint: 605
2025-05-18 00:42:55,874 - app - INFO - [inference_service.py:87] - Model initialized
2025-05-18 00:42:55,885 - app - INFO - [inference_service.py:92] - Model weights loaded
2025-05-18 00:42:55,886 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 00:42:56,204 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-18 00:42:56,218 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-18 00:42:56,233 - app - INFO - [model_trainer.py:103] - Train data size: 23973, Validation data size: 2664
2025-05-18 00:42:56,233 - app - INFO - [inference_service.py:104] - Using custom tokenizer with vocab size: 605
2025-05-18 00:42:56,236 - app - INFO - [inference_service.py:116] - Temporary file cleaned up
2025-05-18 00:42:56,239 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:42:56,239 - app - INFO - [inference_service.py:166] - Prompt: Machine learning is
2025-05-18 00:42:56,239 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:42:56,239 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:42:56,239 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:42:56,239 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:42:56,240 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 3])
2025-05-18 00:42:56,240 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:42:57,343 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:42:57,343 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:42:57,344 - app - INFO - [inference_service.py:166] - Prompt: Neural networks can
2025-05-18 00:42:57,344 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:42:57,344 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:42:57,344 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:42:57,345 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:42:57,345 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 3])
2025-05-18 00:42:57,345 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:42:58,404 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:42:58,405 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:42:58,405 - app - INFO - [inference_service.py:166] - Prompt: Deep learning enables
2025-05-18 00:42:58,405 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:42:58,405 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:42:58,405 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:42:58,405 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:42:58,405 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 3])
2025-05-18 00:42:58,406 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:42:59,384 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:42:59,384 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:42:59,385 - app - INFO - [inference_service.py:166] - Prompt: Reinforcement learning allows
2025-05-18 00:42:59,385 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:42:59,385 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:42:59,385 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:42:59,386 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:42:59,386 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 8])
2025-05-18 00:42:59,386 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:43:00,373 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:43:00,373 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:43:00,374 - app - INFO - [inference_service.py:166] - Prompt: Natural language processing helps
2025-05-18 00:43:00,374 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:43:00,374 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:43:00,375 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:43:00,375 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:43:00,375 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 14])
2025-05-18 00:43:00,375 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:43:01,987 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:43:02,135 - app - INFO - [inference_service.py:22] - Initializing InferenceService
2025-05-18 00:43:02,135 - app - INFO - [inference_service.py:25] - Using device: cpu
2025-05-18 00:43:02,139 - app - INFO - [llm_controller.py:111] - Received inference request for model 57
2025-05-18 00:43:02,139 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:43:02,140 - app - INFO - [inference_service.py:166] - Prompt: Machine learning is a field of study that
2025-05-18 00:43:02,140 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:43:02,140 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:43:02,149 - app - INFO - [inference_service.py:54] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:43:02,535 - app - INFO - [inference_service.py:63] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmpm9qr3xvg.pt
2025-05-18 00:43:02,552 - app - INFO - [inference_service.py:67] - Model checkpoint loaded
2025-05-18 00:43:02,552 - app - INFO - [inference_service.py:71] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:43:02,553 - app - INFO - [inference_service.py:75] - Model vocabulary size from checkpoint: 605
2025-05-18 00:43:02,640 - app - INFO - [inference_service.py:87] - Model initialized
2025-05-18 00:43:02,648 - app - INFO - [inference_service.py:92] - Model weights loaded
2025-05-18 00:43:02,649 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 00:43:02,650 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-18 00:43:02,664 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-18 00:43:02,682 - app - INFO - [model_trainer.py:103] - Train data size: 23973, Validation data size: 2664
2025-05-18 00:43:02,683 - app - INFO - [inference_service.py:104] - Using custom tokenizer with vocab size: 605
2025-05-18 00:43:02,686 - app - INFO - [inference_service.py:116] - Temporary file cleaned up
2025-05-18 00:43:02,688 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:43:02,689 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 12])
2025-05-18 00:43:02,689 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:43:03,705 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:43:03,706 - app - INFO - [llm_controller.py:125] - Inference completed successfully
2025-05-18 00:43:26,689 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:43:26,698 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:44:29,975 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:44:29,989 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:44:49,710 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:44:49,720 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:45:57,663 - app - INFO - [inference_service.py:22] - Initializing InferenceService
2025-05-18 00:45:57,664 - app - INFO - [inference_service.py:25] - Using device: cpu
2025-05-18 00:45:57,666 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:45:57,672 - app - INFO - [inference_service.py:54] - Downloading model from MinIO: models/57/model.pt
2025-05-18 00:45:58,022 - app - INFO - [inference_service.py:63] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmpc2adzkrp.pt
2025-05-18 00:45:58,039 - app - INFO - [inference_service.py:67] - Model checkpoint loaded
2025-05-18 00:45:58,039 - app - INFO - [inference_service.py:71] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 00:45:58,040 - app - INFO - [inference_service.py:75] - Model vocabulary size from checkpoint: 605
2025-05-18 00:45:58,126 - app - INFO - [inference_service.py:87] - Model initialized
2025-05-18 00:45:58,132 - app - INFO - [inference_service.py:92] - Model weights loaded
2025-05-18 00:45:58,132 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 00:45:58,466 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-18 00:45:58,479 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-18 00:45:58,492 - app - INFO - [model_trainer.py:103] - Train data size: 23973, Validation data size: 2664
2025-05-18 00:45:58,493 - app - INFO - [inference_service.py:104] - Using custom tokenizer with vocab size: 605
2025-05-18 00:45:58,495 - app - INFO - [inference_service.py:116] - Temporary file cleaned up
2025-05-18 00:45:58,498 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:45:58,498 - app - INFO - [inference_service.py:166] - Prompt: Machine learning is
2025-05-18 00:45:58,498 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:45:58,499 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:45:58,499 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:45:58,499 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:45:58,500 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 3])
2025-05-18 00:45:58,500 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:45:59,569 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:45:59,569 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:45:59,569 - app - INFO - [inference_service.py:166] - Prompt: Neural networks can
2025-05-18 00:45:59,570 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:45:59,570 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:45:59,570 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:45:59,570 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:45:59,571 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 3])
2025-05-18 00:45:59,571 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:46:00,580 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:46:00,581 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:46:00,581 - app - INFO - [inference_service.py:166] - Prompt: Deep learning enables
2025-05-18 00:46:00,581 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:46:00,581 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:46:00,581 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:46:00,582 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:46:00,582 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 3])
2025-05-18 00:46:00,582 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:46:01,911 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:46:01,912 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:46:01,912 - app - INFO - [inference_service.py:166] - Prompt: Reinforcement learning allows
2025-05-18 00:46:01,912 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:46:01,913 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:46:01,913 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:46:01,913 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:46:01,913 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 8])
2025-05-18 00:46:01,914 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:46:03,159 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:46:03,160 - app - INFO - [inference_service.py:165] - Generating response for model 57
2025-05-18 00:46:03,160 - app - INFO - [inference_service.py:166] - Prompt: Natural language processing helps
2025-05-18 00:46:03,160 - app - INFO - [inference_service.py:167] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 00:46:03,161 - app - INFO - [inference_service.py:34] - Loading model 57
2025-05-18 00:46:03,161 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 00:46:03,161 - app - INFO - [inference_service.py:181] - Encoding input
2025-05-18 00:46:03,161 - app - INFO - [inference_service.py:184] - Input shape: torch.Size([1, 14])
2025-05-18 00:46:03,162 - app - INFO - [inference_service.py:191] - Generating response
2025-05-18 00:46:04,898 - app - INFO - [inference_service.py:232] - Decoding response
2025-05-18 00:46:24,438 - app - INFO - [llm_controller.py:73] - Model 58 created and training started
2025-05-18 00:46:24,457 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 00:46:24,510 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-18 00:46:24,531 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-18 00:46:24,550 - app - INFO - [model_trainer.py:103] - Train data size: 23973, Validation data size: 2664
2025-05-18 00:46:24,582 - app - ERROR - [llm_controller.py:37] - Training failed for model 58: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:
 * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
 * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)

2025-05-18 00:47:02,981 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:47:02,998 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:47:22,470 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 00:47:22,485 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 00:47:22,550 - app - INFO - [llm_controller.py:73] - Model 59 created and training started
2025-05-18 00:47:22,572 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 00:47:22,970 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-18 00:47:22,987 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-18 00:47:23,000 - app - INFO - [model_trainer.py:103] - Train data size: 23973, Validation data size: 2664
2025-05-18 01:12:09,136 - app - INFO - [model_trainer.py:163] - Early stopping triggered
2025-05-18 01:12:19,624 - app - INFO - [inference_service.py:22] - Initializing InferenceService
2025-05-18 01:12:19,625 - app - INFO - [inference_service.py:25] - Using device: cpu
2025-05-18 01:12:19,639 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:12:19,645 - app - INFO - [inference_service.py:54] - Downloading model from MinIO: models/59/model.pt
2025-05-18 01:12:19,996 - app - INFO - [inference_service.py:63] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmps0zpuwk7.pt
2025-05-18 01:12:20,020 - app - INFO - [inference_service.py:67] - Model checkpoint loaded
2025-05-18 01:12:20,020 - app - INFO - [inference_service.py:71] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 01:12:20,020 - app - INFO - [inference_service.py:75] - Model vocabulary size from checkpoint: 605
2025-05-18 01:12:20,102 - app - INFO - [inference_service.py:87] - Model initialized
2025-05-18 01:12:20,112 - app - INFO - [inference_service.py:92] - Model weights loaded
2025-05-18 01:12:20,112 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 01:12:20,114 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-18 01:12:20,129 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-18 01:12:20,143 - app - INFO - [model_trainer.py:103] - Train data size: 23973, Validation data size: 2664
2025-05-18 01:12:20,144 - app - INFO - [inference_service.py:107] - Using custom tokenizer with vocab size: 605
2025-05-18 01:12:20,145 - app - INFO - [inference_service.py:119] - Temporary file cleaned up
2025-05-18 01:12:20,147 - app - INFO - [inference_service.py:168] - Generating response for model 59
2025-05-18 01:12:20,148 - app - INFO - [inference_service.py:169] - Prompt: Machine learning is
2025-05-18 01:12:20,148 - app - INFO - [inference_service.py:170] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:12:20,148 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:12:20,148 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:12:20,149 - app - INFO - [inference_service.py:184] - Encoding input
2025-05-18 01:12:20,149 - app - ERROR - [inference_service.py:190] - Error encoding input: 'NoneType' object has no attribute 'encode'
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 186, in generate_response
    input_ids = torch.tensor([self.tokenizer.encode(prompt)]).to(self.device)
AttributeError: 'NoneType' object has no attribute 'encode'
2025-05-18 01:12:20,154 - app - INFO - [inference_service.py:168] - Generating response for model 59
2025-05-18 01:12:20,154 - app - INFO - [inference_service.py:169] - Prompt: Neural networks can
2025-05-18 01:12:20,154 - app - INFO - [inference_service.py:170] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:12:20,155 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:12:20,155 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:12:20,155 - app - INFO - [inference_service.py:184] - Encoding input
2025-05-18 01:12:20,156 - app - ERROR - [inference_service.py:190] - Error encoding input: 'NoneType' object has no attribute 'encode'
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 186, in generate_response
    input_ids = torch.tensor([self.tokenizer.encode(prompt)]).to(self.device)
AttributeError: 'NoneType' object has no attribute 'encode'
2025-05-18 01:12:20,157 - app - INFO - [inference_service.py:168] - Generating response for model 59
2025-05-18 01:12:20,157 - app - INFO - [inference_service.py:169] - Prompt: Deep learning enables
2025-05-18 01:12:20,157 - app - INFO - [inference_service.py:170] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:12:20,157 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:12:20,158 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:12:20,158 - app - INFO - [inference_service.py:184] - Encoding input
2025-05-18 01:12:20,158 - app - ERROR - [inference_service.py:190] - Error encoding input: 'NoneType' object has no attribute 'encode'
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 186, in generate_response
    input_ids = torch.tensor([self.tokenizer.encode(prompt)]).to(self.device)
AttributeError: 'NoneType' object has no attribute 'encode'
2025-05-18 01:12:20,159 - app - INFO - [inference_service.py:168] - Generating response for model 59
2025-05-18 01:12:20,159 - app - INFO - [inference_service.py:169] - Prompt: Reinforcement learning allows
2025-05-18 01:12:20,160 - app - INFO - [inference_service.py:170] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:12:20,160 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:12:20,160 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:12:20,160 - app - INFO - [inference_service.py:184] - Encoding input
2025-05-18 01:12:20,160 - app - ERROR - [inference_service.py:190] - Error encoding input: 'NoneType' object has no attribute 'encode'
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 186, in generate_response
    input_ids = torch.tensor([self.tokenizer.encode(prompt)]).to(self.device)
AttributeError: 'NoneType' object has no attribute 'encode'
2025-05-18 01:12:20,161 - app - INFO - [inference_service.py:168] - Generating response for model 59
2025-05-18 01:12:20,162 - app - INFO - [inference_service.py:169] - Prompt: Natural language processing helps
2025-05-18 01:12:20,162 - app - INFO - [inference_service.py:170] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:12:20,162 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:12:20,162 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:12:20,163 - app - INFO - [inference_service.py:184] - Encoding input
2025-05-18 01:12:20,163 - app - ERROR - [inference_service.py:190] - Error encoding input: 'NoneType' object has no attribute 'encode'
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 186, in generate_response
    input_ids = torch.tensor([self.tokenizer.encode(prompt)]).to(self.device)
AttributeError: 'NoneType' object has no attribute 'encode'
2025-05-18 01:12:20,233 - app - INFO - [inference_service.py:22] - Initializing InferenceService
2025-05-18 01:12:20,233 - app - INFO - [inference_service.py:25] - Using device: cpu
2025-05-18 01:12:20,238 - app - INFO - [llm_controller.py:111] - Received inference request for model 59
2025-05-18 01:12:20,238 - app - INFO - [inference_service.py:168] - Generating response for model 59
2025-05-18 01:12:20,238 - app - INFO - [inference_service.py:169] - Prompt: Machine learning is a field of study that
2025-05-18 01:12:20,239 - app - INFO - [inference_service.py:170] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:12:20,239 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:12:20,247 - app - INFO - [inference_service.py:54] - Downloading model from MinIO: models/59/model.pt
2025-05-18 01:12:20,644 - app - INFO - [inference_service.py:63] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmp0koyq72t.pt
2025-05-18 01:12:20,662 - app - INFO - [inference_service.py:67] - Model checkpoint loaded
2025-05-18 01:12:20,663 - app - INFO - [inference_service.py:71] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 01:12:20,663 - app - INFO - [inference_service.py:75] - Model vocabulary size from checkpoint: 605
2025-05-18 01:12:20,745 - app - INFO - [inference_service.py:87] - Model initialized
2025-05-18 01:12:20,754 - app - INFO - [inference_service.py:92] - Model weights loaded
2025-05-18 01:12:20,755 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 01:12:20,756 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-18 01:12:20,773 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-18 01:12:20,786 - app - INFO - [model_trainer.py:103] - Train data size: 23973, Validation data size: 2664
2025-05-18 01:12:20,787 - app - INFO - [inference_service.py:107] - Using custom tokenizer with vocab size: 605
2025-05-18 01:12:20,788 - app - INFO - [inference_service.py:119] - Temporary file cleaned up
2025-05-18 01:12:20,791 - app - INFO - [inference_service.py:184] - Encoding input
2025-05-18 01:12:20,792 - app - ERROR - [inference_service.py:190] - Error encoding input: 'NoneType' object has no attribute 'encode'
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/service/inference_service.py", line 186, in generate_response
    input_ids = torch.tensor([self.tokenizer.encode(prompt)]).to(self.device)
AttributeError: 'NoneType' object has no attribute 'encode'
2025-05-18 01:12:20,793 - app - ERROR - [llm_controller.py:122] - Inference error: Error encoding input: 'NoneType' object has no attribute 'encode'
2025-05-18 01:12:20,793 - app - ERROR - [llm_controller.py:133] - Error during inference: 500: Error encoding input: 'NoneType' object has no attribute 'encode'
Traceback (most recent call last):
  File "/Users/alvarobrito/desenvolvimento/projetos/github-alvaro-brito/private-projects/llm-by-zero/app/controller/llm_controller.py", line 123, in generate_inference
    raise HTTPException(status_code=500, detail=response["error"])
fastapi.exceptions.HTTPException: 500: Error encoding input: 'NoneType' object has no attribute 'encode'
2025-05-18 01:13:06,133 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 01:13:06,147 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 01:13:06,258 - app - INFO - [inference_service.py:22] - Initializing InferenceService
2025-05-18 01:13:06,259 - app - INFO - [inference_service.py:25] - Using device: cpu
2025-05-18 01:13:06,263 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:13:06,274 - app - INFO - [inference_service.py:54] - Downloading model from MinIO: models/59/model.pt
2025-05-18 01:13:06,700 - app - INFO - [inference_service.py:63] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmprtzl8vxw.pt
2025-05-18 01:13:06,723 - app - INFO - [inference_service.py:67] - Model checkpoint loaded
2025-05-18 01:13:06,723 - app - INFO - [inference_service.py:71] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 01:13:06,724 - app - INFO - [inference_service.py:75] - Model vocabulary size from checkpoint: 605
2025-05-18 01:13:06,814 - app - INFO - [inference_service.py:87] - Model initialized
2025-05-18 01:13:06,823 - app - INFO - [inference_service.py:92] - Model weights loaded
2025-05-18 01:13:06,823 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 01:13:07,143 - app - INFO - [inference_service.py:163] - Using custom tokenizer with vocab size: 605
2025-05-18 01:13:07,144 - app - INFO - [inference_service.py:175] - Temporary file cleaned up
2025-05-18 01:13:07,147 - app - INFO - [inference_service.py:224] - Generating response for model 59
2025-05-18 01:13:07,147 - app - INFO - [inference_service.py:225] - Prompt: Machine learning is
2025-05-18 01:13:07,147 - app - INFO - [inference_service.py:226] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:13:07,147 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:13:07,148 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:13:07,148 - app - INFO - [inference_service.py:240] - Encoding input
2025-05-18 01:13:07,148 - app - INFO - [inference_service.py:243] - Input shape: torch.Size([1, 3])
2025-05-18 01:13:07,148 - app - INFO - [inference_service.py:250] - Generating response
2025-05-18 01:13:08,246 - app - INFO - [inference_service.py:291] - Decoding response
2025-05-18 01:13:08,247 - app - INFO - [inference_service.py:224] - Generating response for model 59
2025-05-18 01:13:08,248 - app - INFO - [inference_service.py:225] - Prompt: Neural networks can
2025-05-18 01:13:08,248 - app - INFO - [inference_service.py:226] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:13:08,248 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:13:08,249 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:13:08,249 - app - INFO - [inference_service.py:240] - Encoding input
2025-05-18 01:13:08,249 - app - INFO - [inference_service.py:243] - Input shape: torch.Size([1, 3])
2025-05-18 01:13:08,249 - app - INFO - [inference_service.py:250] - Generating response
2025-05-18 01:13:09,124 - app - INFO - [inference_service.py:291] - Decoding response
2025-05-18 01:13:09,124 - app - INFO - [inference_service.py:224] - Generating response for model 59
2025-05-18 01:13:09,125 - app - INFO - [inference_service.py:225] - Prompt: Deep learning enables
2025-05-18 01:13:09,125 - app - INFO - [inference_service.py:226] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:13:09,125 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:13:09,125 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:13:09,125 - app - INFO - [inference_service.py:240] - Encoding input
2025-05-18 01:13:09,126 - app - INFO - [inference_service.py:243] - Input shape: torch.Size([1, 3])
2025-05-18 01:13:09,126 - app - INFO - [inference_service.py:250] - Generating response
2025-05-18 01:13:09,985 - app - INFO - [inference_service.py:291] - Decoding response
2025-05-18 01:13:09,986 - app - INFO - [inference_service.py:224] - Generating response for model 59
2025-05-18 01:13:09,986 - app - INFO - [inference_service.py:225] - Prompt: Reinforcement learning allows
2025-05-18 01:13:09,986 - app - INFO - [inference_service.py:226] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:13:09,986 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:13:09,986 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:13:09,986 - app - INFO - [inference_service.py:240] - Encoding input
2025-05-18 01:13:09,987 - app - INFO - [inference_service.py:243] - Input shape: torch.Size([1, 8])
2025-05-18 01:13:09,987 - app - INFO - [inference_service.py:250] - Generating response
2025-05-18 01:13:10,835 - app - INFO - [inference_service.py:291] - Decoding response
2025-05-18 01:13:10,836 - app - INFO - [inference_service.py:224] - Generating response for model 59
2025-05-18 01:13:10,836 - app - INFO - [inference_service.py:225] - Prompt: Natural language processing helps
2025-05-18 01:13:10,836 - app - INFO - [inference_service.py:226] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:13:10,836 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:13:10,836 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:13:10,837 - app - INFO - [inference_service.py:240] - Encoding input
2025-05-18 01:13:10,837 - app - INFO - [inference_service.py:243] - Input shape: torch.Size([1, 14])
2025-05-18 01:13:10,837 - app - INFO - [inference_service.py:250] - Generating response
2025-05-18 01:13:11,656 - app - INFO - [inference_service.py:291] - Decoding response
2025-05-18 01:13:16,690 - app.main - INFO - [main.py:44] - Starting up FastAPI application
2025-05-18 01:13:16,700 - app.main - INFO - [main.py:48] - Database connection successful
2025-05-18 01:13:16,718 - app - INFO - [inference_service.py:22] - Initializing InferenceService
2025-05-18 01:13:16,718 - app - INFO - [inference_service.py:25] - Using device: cpu
2025-05-18 01:13:16,721 - app - INFO - [llm_controller.py:111] - Received inference request for model 59
2025-05-18 01:13:16,721 - app - INFO - [inference_service.py:224] - Generating response for model 59
2025-05-18 01:13:16,721 - app - INFO - [inference_service.py:225] - Prompt: Machine learning is a field of study that
2025-05-18 01:13:16,722 - app - INFO - [inference_service.py:226] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:13:16,722 - app - INFO - [inference_service.py:34] - Loading model 59
2025-05-18 01:13:16,737 - app - INFO - [inference_service.py:54] - Downloading model from MinIO: models/59/model.pt
2025-05-18 01:13:17,188 - app - INFO - [inference_service.py:63] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmp8nslbj42.pt
2025-05-18 01:13:17,209 - app - INFO - [inference_service.py:67] - Model checkpoint loaded
2025-05-18 01:13:17,209 - app - INFO - [inference_service.py:71] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 01:13:17,209 - app - INFO - [inference_service.py:75] - Model vocabulary size from checkpoint: 605
2025-05-18 01:13:17,307 - app - INFO - [inference_service.py:87] - Model initialized
2025-05-18 01:13:17,318 - app - INFO - [inference_service.py:92] - Model weights loaded
2025-05-18 01:13:17,319 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 01:13:17,640 - app - INFO - [inference_service.py:163] - Using custom tokenizer with vocab size: 605
2025-05-18 01:13:17,642 - app - INFO - [inference_service.py:175] - Temporary file cleaned up
2025-05-18 01:13:17,644 - app - INFO - [inference_service.py:240] - Encoding input
2025-05-18 01:13:17,645 - app - INFO - [inference_service.py:243] - Input shape: torch.Size([1, 12])
2025-05-18 01:13:17,645 - app - INFO - [inference_service.py:250] - Generating response
2025-05-18 01:13:18,635 - app - INFO - [inference_service.py:291] - Decoding response
2025-05-18 01:13:18,653 - app - INFO - [llm_controller.py:125] - Inference completed successfully
2025-05-18 01:15:05,035 - app - INFO - [llm_controller.py:73] - Model 60 created and training started
2025-05-18 01:15:05,048 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 01:15:05,088 - app - INFO - [model_trainer.py:32] - Preparing training data
2025-05-18 01:15:05,102 - app - INFO - [model_trainer.py:67] - Vocabulary size: 605
2025-05-18 01:15:05,190 - app - INFO - [model_trainer.py:103] - Train data size: 23973, Validation data size: 2664
2025-05-18 01:42:56,559 - app - INFO - [model_trainer.py:163] - Early stopping triggered
2025-05-18 01:43:05,268 - app - INFO - [inference_service.py:22] - Initializing InferenceService
2025-05-18 01:43:05,270 - app - INFO - [inference_service.py:25] - Using device: cpu
2025-05-18 01:43:05,295 - app - INFO - [inference_service.py:34] - Loading model 60
2025-05-18 01:43:05,305 - app - INFO - [inference_service.py:54] - Downloading model from MinIO: models/60/model.pt
2025-05-18 01:43:06,065 - app - INFO - [inference_service.py:63] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmpwi2n3j43.pt
2025-05-18 01:43:06,097 - app - INFO - [inference_service.py:67] - Model checkpoint loaded
2025-05-18 01:43:06,097 - app - INFO - [inference_service.py:71] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 01:43:06,098 - app - INFO - [inference_service.py:75] - Model vocabulary size from checkpoint: 605
2025-05-18 01:43:06,246 - app - INFO - [inference_service.py:87] - Model initialized
2025-05-18 01:43:06,258 - app - INFO - [inference_service.py:92] - Model weights loaded
2025-05-18 01:43:06,259 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 01:43:06,285 - app - INFO - [inference_service.py:163] - Using custom tokenizer with vocab size: 605
2025-05-18 01:43:06,286 - app - INFO - [inference_service.py:175] - Temporary file cleaned up
2025-05-18 01:43:06,291 - app - INFO - [inference_service.py:224] - Generating response for model 60
2025-05-18 01:43:06,292 - app - INFO - [inference_service.py:225] - Prompt: Machine learning is
2025-05-18 01:43:06,292 - app - INFO - [inference_service.py:226] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:43:06,292 - app - INFO - [inference_service.py:34] - Loading model 60
2025-05-18 01:43:06,293 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:43:06,293 - app - INFO - [inference_service.py:240] - Encoding input
2025-05-18 01:43:06,293 - app - INFO - [inference_service.py:243] - Input shape: torch.Size([1, 3])
2025-05-18 01:43:06,293 - app - INFO - [inference_service.py:250] - Generating response
2025-05-18 01:43:08,039 - app - INFO - [inference_service.py:291] - Decoding response
2025-05-18 01:43:08,039 - app - INFO - [inference_service.py:224] - Generating response for model 60
2025-05-18 01:43:08,040 - app - INFO - [inference_service.py:225] - Prompt: Neural networks can
2025-05-18 01:43:08,040 - app - INFO - [inference_service.py:226] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:43:08,040 - app - INFO - [inference_service.py:34] - Loading model 60
2025-05-18 01:43:08,040 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:43:08,041 - app - INFO - [inference_service.py:240] - Encoding input
2025-05-18 01:43:08,041 - app - INFO - [inference_service.py:243] - Input shape: torch.Size([1, 3])
2025-05-18 01:43:08,041 - app - INFO - [inference_service.py:250] - Generating response
2025-05-18 01:43:09,374 - app - INFO - [inference_service.py:291] - Decoding response
2025-05-18 01:43:09,374 - app - INFO - [inference_service.py:224] - Generating response for model 60
2025-05-18 01:43:09,375 - app - INFO - [inference_service.py:225] - Prompt: Deep learning enables
2025-05-18 01:43:09,375 - app - INFO - [inference_service.py:226] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:43:09,375 - app - INFO - [inference_service.py:34] - Loading model 60
2025-05-18 01:43:09,375 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:43:09,376 - app - INFO - [inference_service.py:240] - Encoding input
2025-05-18 01:43:09,376 - app - INFO - [inference_service.py:243] - Input shape: torch.Size([1, 3])
2025-05-18 01:43:09,376 - app - INFO - [inference_service.py:250] - Generating response
2025-05-18 01:43:10,623 - app - INFO - [inference_service.py:291] - Decoding response
2025-05-18 01:43:10,623 - app - INFO - [inference_service.py:224] - Generating response for model 60
2025-05-18 01:43:10,623 - app - INFO - [inference_service.py:225] - Prompt: Reinforcement learning allows
2025-05-18 01:43:10,623 - app - INFO - [inference_service.py:226] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:43:10,624 - app - INFO - [inference_service.py:34] - Loading model 60
2025-05-18 01:43:10,624 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:43:10,624 - app - INFO - [inference_service.py:240] - Encoding input
2025-05-18 01:43:10,624 - app - INFO - [inference_service.py:243] - Input shape: torch.Size([1, 8])
2025-05-18 01:43:10,625 - app - INFO - [inference_service.py:250] - Generating response
2025-05-18 01:43:11,743 - app - INFO - [inference_service.py:291] - Decoding response
2025-05-18 01:43:11,744 - app - INFO - [inference_service.py:224] - Generating response for model 60
2025-05-18 01:43:11,744 - app - INFO - [inference_service.py:225] - Prompt: Natural language processing helps
2025-05-18 01:43:11,744 - app - INFO - [inference_service.py:226] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:43:11,744 - app - INFO - [inference_service.py:34] - Loading model 60
2025-05-18 01:43:11,745 - app - INFO - [inference_service.py:38] - Model already loaded
2025-05-18 01:43:11,745 - app - INFO - [inference_service.py:240] - Encoding input
2025-05-18 01:43:11,745 - app - INFO - [inference_service.py:243] - Input shape: torch.Size([1, 14])
2025-05-18 01:43:11,745 - app - INFO - [inference_service.py:250] - Generating response
2025-05-18 01:43:12,729 - app - INFO - [inference_service.py:291] - Decoding response
2025-05-18 01:43:12,864 - app - INFO - [inference_service.py:22] - Initializing InferenceService
2025-05-18 01:43:12,865 - app - INFO - [inference_service.py:25] - Using device: cpu
2025-05-18 01:43:12,866 - app - INFO - [llm_controller.py:111] - Received inference request for model 60
2025-05-18 01:43:12,866 - app - INFO - [inference_service.py:224] - Generating response for model 60
2025-05-18 01:43:12,866 - app - INFO - [inference_service.py:225] - Prompt: Machine learning is a field of study that
2025-05-18 01:43:12,867 - app - INFO - [inference_service.py:226] - Parameters: max_length=100, temperature=0.7, top_p=0.9, num_sequences=1
2025-05-18 01:43:12,867 - app - INFO - [inference_service.py:34] - Loading model 60
2025-05-18 01:43:12,876 - app - INFO - [inference_service.py:54] - Downloading model from MinIO: models/60/model.pt
2025-05-18 01:43:13,239 - app - INFO - [inference_service.py:63] - Model downloaded to temporary file: /var/folders/_p/81c319n511524sm5p0qktsgc0000gp/T/tmpgk_2f01k.pt
2025-05-18 01:43:13,263 - app - INFO - [inference_service.py:67] - Model checkpoint loaded
2025-05-18 01:43:13,263 - app - INFO - [inference_service.py:71] - Model config: {'batch_size': 128, 'context_length': 128, 'd_model': 256, 'num_blocks': 4, 'num_heads': 4, 'learning_rate': 0.0003, 'dropout': 0.1}
2025-05-18 01:43:13,263 - app - INFO - [inference_service.py:75] - Model vocabulary size from checkpoint: 605
2025-05-18 01:43:13,347 - app - INFO - [inference_service.py:87] - Model initialized
2025-05-18 01:43:13,356 - app - INFO - [inference_service.py:92] - Model weights loaded
2025-05-18 01:43:13,356 - app - INFO - [model_trainer.py:27] - ModelTrainer initialized with device: cpu
2025-05-18 01:43:13,373 - app - INFO - [inference_service.py:163] - Using custom tokenizer with vocab size: 605
2025-05-18 01:43:13,375 - app - INFO - [inference_service.py:175] - Temporary file cleaned up
2025-05-18 01:43:13,378 - app - INFO - [inference_service.py:240] - Encoding input
2025-05-18 01:43:13,378 - app - INFO - [inference_service.py:243] - Input shape: torch.Size([1, 12])
2025-05-18 01:43:13,378 - app - INFO - [inference_service.py:250] - Generating response
2025-05-18 01:43:14,332 - app - INFO - [inference_service.py:291] - Decoding response
2025-05-18 01:43:14,332 - app - INFO - [llm_controller.py:125] - Inference completed successfully
