# LLM-FROM-ZERO: Build Your Own Language Model From Scratch

A complete pipeline for training, evaluating, and deploying transformer-based language models  GPT-like from scratch. This project provides a modular and scalable architecture for experimenting with custom language models.

## ğŸŒŸ Features

- **Complete Training Pipeline**: End-to-end solution for training transformer models
- **Custom Tokenization**: Adaptive word and character-level tokenization
- **Real-time Monitoring**: Track training progress and model metrics
- **Model Evaluation**: Built-in evaluation metrics including perplexity
- **REST API**: Easy-to-use interface for model inference
- **MinIO Integration**: Efficient model storage and versioning
- **Docker Support**: Containerized deployment for all components

## ğŸ—ï¸ Architecture

The project consists of several key components:

- **Training Service**: Handles model training and checkpointing
- **Inference Service**: Manages model loading and text generation
- **Model Storage**: MinIO-based artifact storage
- **Database**: PostgreSQL for model metadata and status tracking
- **REST API**: FastAPI-based endpoint for model interaction
- **Monitoring**: Real-time training progress and metrics tracking

### Training Flow
```
                                    Training Pipeline
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚    â”‚   Training   â”‚    â”‚              â”‚    â”‚              â”‚
â”‚  Input Text  â”‚â”€â”€â”€>â”‚     Data     â”‚â”€â”€â”€>â”‚ Tokenization â”‚â”€â”€â”€>â”‚  Vocabulary  â”‚
â”‚              â”‚    â”‚  Processing  â”‚    â”‚              â”‚    â”‚  Creation    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â–¼
â”‚              â”‚    â”‚              â”‚    â”‚              â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Model     â”‚<â”€â”€â”€â”‚   Training   â”‚<â”€â”€â”€â”‚    Batch     â”‚<â”€â”€â”€â”‚  DataLoader  â”‚
â”‚ Checkpointingâ”‚    â”‚    Loop      â”‚    â”‚  Generation  â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚
       â”‚                   â”‚
       â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MinIO     â”‚    â”‚   Progress   â”‚
â”‚   Storage    â”‚    â”‚  Monitoring  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Evaluation Flow
```
                                    Evaluation Pipeline
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚    â”‚              â”‚    â”‚  Perplexity  â”‚    â”‚              â”‚
â”‚ Load Model   â”‚â”€â”€â”€>â”‚  Test Data   â”‚â”€â”€â”€>â”‚ Calculation  â”‚â”€â”€â”€>â”‚   Metrics    â”‚
â”‚              â”‚    â”‚              â”‚    â”‚              â”‚    â”‚ Aggregation  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²                                                            â”‚
       â”‚                                                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â–¼
â”‚              â”‚                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MinIO     â”‚                                           â”‚    JSON      â”‚
â”‚   Storage    â”‚                                           â”‚   Report     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Inference Flow
```
                                    Inference Pipeline
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚    â”‚              â”‚    â”‚              â”‚    â”‚              â”‚
â”‚   Prompt     â”‚â”€â”€â”€>â”‚ Tokenization â”‚â”€â”€â”€>â”‚   Model      â”‚â”€â”€â”€>â”‚   Token      â”‚
â”‚              â”‚    â”‚              â”‚    â”‚  Inference   â”‚    â”‚ Generation   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–²                    â–²                    â”‚
                           â”‚                    â”‚                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â–¼
                    â”‚              â”‚    â”‚              â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Vocabulary  â”‚    â”‚ Load Model   â”‚    â”‚    Text      â”‚
                    â”‚              â”‚    â”‚              â”‚    â”‚  Decoding    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–²                    â–²                    â”‚
                           â”‚                    â”‚                    â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              â”‚                         â”‚              â”‚
                    â”‚    MinIO     â”‚                         â”‚   Response   â”‚
                    â”‚   Storage    â”‚                         â”‚              â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- Docker and Docker Compose
- MinIO Client (mc)
- PostgreSQL

### Installation

1. Clone the repository:
```bash
git clone https://github.com/alvaro-brito/llm-from-zero.git
cd llm-by-zero
```

2. Set up the virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Start the services:
```bash
docker-compose up -d
```

4. Initialize the database and the microservice:
```bash
./start_backend.sh
```

### Training a Model

Use the training script to create and train a new model:

```bash
./train_and_evaluate.sh
```

This will:
1. Create a new model instance
2. Start the training process
3. Monitor training progress
4. Evaluate the model
5. Run inference tests

### Model Evaluation

The system evaluates models using several metrics:
- Perplexity
- Response length statistics
- Processing time
- Sample quality

### Inference

Models can be used for inference through:

1. REST API:
```bash
curl -X POST "http://localhost:8000/api/v1/models/{model_id}/generate" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "Your prompt here", "max_length": 100}'
```

2. Command line:
```bash
./train_and_evaluate.sh --model <model_id>
```

## ğŸ”§ Configuration

Key configuration parameters in `app/core/config.py`:

- `BATCH_SIZE`: Training batch size
- `CONTEXT_LENGTH`: Maximum sequence length
- `D_MODEL`: Model embedding dimension
- `NUM_HEADS`: Number of attention heads
- `NUM_BLOCKS`: Number of transformer blocks
- `LEARNING_RATE`: Training learning rate
- `DROPOUT`: Dropout rate
- `MAX_ITERS`: Maximum training iterations

## ğŸ“Š Model Architecture

The transformer model includes:
- Multi-head self-attention
- Position-wise feed-forward networks
- Layer normalization
- Residual connections
- Adaptive tokenization

### Technical Implementation Details

#### 1. Tokenization and Embedding
```
                   Tokenization Pipeline
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input      â”‚    â”‚  Word-level  â”‚    â”‚  Character   â”‚
â”‚    Text      â”‚â”€â”€â”€>â”‚ Tokenization â”‚â”€â”€â”€>â”‚    Level     â”‚
â”‚              â”‚    â”‚              â”‚    â”‚ Tokenization â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                    â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
                                          â–¼     â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚  Vocabulary  â”‚
                                   â”‚  Creation    â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- Custom tokenizer combining word and character-level tokenization
- Special tokens handling (`<|endoftext|>`, `<|pad|>`, `<|unk|>`)
- Dynamic vocabulary creation based on frequency

#### 2. Training Process
```
                    Training Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Token      â”‚    â”‚  Positional  â”‚    â”‚   Input      â”‚
â”‚  Embedding   â”‚â”€â”€â”€>â”‚  Encoding    â”‚â”€â”€â”€>â”‚  Embedding   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                      â”‚
        â”‚           Transformer Block          â”‚
        â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  Multi-Head  â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  Attention   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Layer     â”‚
                    â”‚    Norm      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     FFN      â”‚
                    â”‚   Network    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Layer     â”‚
                    â”‚    Norm      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Output    â”‚
                    â”‚   Logits     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Key Components:
- Token Embedding: `nn.Embedding(vocab_size, d_model)`
- Positional Encoding: Sine/cosine based
- Multi-head Attention:
  ```python
  Q = Wq(X)  # [batch_size, seq_len, d_model]
  K = Wk(X)  # [batch_size, seq_len, d_model]
  V = Wv(X)  # [batch_size, seq_len, d_model]
  attention = softmax(QK^T/sqrt(d_k))V
  ```
- Feed Forward Network:
  ```python
  FFN(x) = max(0, xW1 + b1)W2 + b2
  ```

#### 3. Inference Pipeline
```
                    Inference Flow
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input      â”‚    â”‚  Tokenize &  â”‚    â”‚   Model      â”‚
â”‚   Prompt     â”‚â”€â”€â”€>â”‚   Embed      â”‚â”€â”€â”€>â”‚  Forward     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   Final      â”‚    â”‚   Sample     â”‚          â”‚
â”‚   Text       â”‚<â”€â”€â”€â”‚   Token      â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Generation Strategy:
- Temperature-based sampling
- Top-p (nucleus) sampling
- Maximum length control
- Special token handling

#### 4. Model Parameters
```
Hyperparameters Configuration
----------------------------
batch_size     = 4      # Training batch size
context_length = 16     # Sequence length
d_model        = 64     # Embedding dimension
num_blocks     = 8      # Transformer blocks
num_heads      = 4      # Attention heads
learning_rate  = 1e-3   # Training rate
dropout        = 0.1    # Dropout rate
max_iters      = 5000   # Training iterations
```

#### 5. Evaluation Metrics
```
                    Evaluation Pipeline
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Model     â”‚    â”‚  Calculate   â”‚    â”‚  Aggregate   â”‚
â”‚  Generation  â”‚â”€â”€â”€>â”‚  Perplexity  â”‚â”€â”€â”€>â”‚   Metrics    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                    â”‚
        â–¼                  â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Response   â”‚    â”‚    Loss      â”‚    â”‚    Final     â”‚
â”‚   Quality    â”‚    â”‚  Analysis    â”‚    â”‚   Report     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Key Metrics:
- Perplexity: exp(average negative log likelihood)
- Response Length Statistics
- Token Distribution Analysis
- Generation Speed

## ğŸ” Monitoring

Training progress can be monitored through:
- Real-time progress updates
- Log files in `logs/` directory
- Model status API endpoint

## ğŸ“ API Documentation

The REST API provides endpoints for:

- Model Management:
  - `POST /api/v1/models/create`: Create new model
  - `GET /api/v1/models/{model_id}/status`: Check model status
  - `GET /api/v1/models`: List all models

- Inference:
  - `POST /api/v1/models/{model_id}/generate`: Generate text
  - `POST /api/v1/models/{model_id}/evaluate`: Evaluate model

## ğŸ› ï¸ Development

### Project Structure

```
llm-by-zero/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/           # API endpoints
â”‚   â”œâ”€â”€ core/          # Core configurations
â”‚   â”œâ”€â”€ db/            # Database models
â”‚   â”œâ”€â”€ helpers/       # Utility functions
â”‚   â””â”€â”€ service/       # Business logic
â”œâ”€â”€ migrations/        # Database migrations
â”œâ”€â”€ models/           # Saved model checkpoints
â”œâ”€â”€ data/            # Training data
â””â”€â”€ docker/          # Docker configurations
```

### Adding New Features

1. Create new endpoints in `app/api/`
2. Implement business logic in `app/service/`
3. Update database models in `app/db/models/`
4. Add migrations using `alembic revision`

## ğŸ“ˆ Performance

Current benchmarks:
- Training time: ~10 minutes for basic model
- Inference latency: ~1.5 seconds per request
- Perplexity: 35-50k range

## ğŸ™ Acknowledgments

- PyTorch team for the deep learning framework
- Transformer architecture paper authors
- FastAPI for the web framework
- MinIO for object storage

## ğŸ“ Support

For support, please:
1. Check the documentation
2. Search existing issues
3. Open a new issue if needed

---
Built with â¤ï¸ using PyTorch and FastAPI 
